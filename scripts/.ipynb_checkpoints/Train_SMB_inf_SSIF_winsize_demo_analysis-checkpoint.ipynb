{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import math\n",
    "import sklearn.preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "import subprocess as sp\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "np.random.seed(4)\n",
    "\n",
    "from models import EarlyStopping, GRU\n",
    "from utilities import get_NSE, MSE, get_colors, print_notes, inverse_transform_sp\n",
    "from preprocess import load_data, SampleData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['deeppink', 'darkviolet', 'darkgray', 'darkblue', 'darkkhaki',\n",
       "       'deepskyblue', 'darkslategray', 'darkgreen', 'darkslategrey',\n",
       "       'darkseagreen', 'darkred', 'darkmagenta', 'darksalmon',\n",
       "       'darkorchid', 'darkorange', 'darkgoldenrod', 'darkturquoise',\n",
       "       'darkolivegreen', 'darkcyan', 'darkgrey', 'darkslateblue'],\n",
       "      dtype='<U14')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_gpu_memory():\n",
    "    _output_to_list = lambda x: x.decode('ascii').split('\\n')[:-1]\n",
    "    ACCEPTABLE_AVAILABLE_MEMORY = 1024\n",
    "    COMMAND = \"nvidia-smi --query-gpu=memory.free --format=csv\"\n",
    "    memory_free_info = _output_to_list(sp.check_output(COMMAND.split()))[1:]\n",
    "    memory_free_values = [int(x.split()[0]) for i, x in enumerate(memory_free_info)]\n",
    "    return memory_free_values\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "if get_gpu_memory()[0] < 1500 :\n",
    "    device = torch.device(\"cpu\")\n",
    "print(device)\n",
    "\n",
    "# Get colors\n",
    "colors = get_colors()\n",
    "colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3441873/245619647.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'params_ss_{}.npy'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mallow_pickle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0mfeat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_vars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_vars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;31m# First 50% of data as the training set, middle 10% of data as the validation set, last 40% of data as the testing set.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/panfs/jay/groups/32/kumarv/xu000114/MPTT_Simulation/scripts_nips/preprocess.py\u001b[0m in \u001b[0;36mload_data\u001b[0;34m(df, output_vars, input_vars, input_size)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m# Convert the dates to the day of the year(DOY), then bend the DOY at day 183 to give DOY a seasonal pattern as shown in figure\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mdates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Date'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mdoy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrptime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcdate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'%Y-%m-%d'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimetuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtm_yday\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcdate\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdates\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mdoy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m183\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoy\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m183\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;31m# Load features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/panfs/jay/groups/32/kumarv/xu000114/MPTT_Simulation/scripts_nips/preprocess.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m# Convert the dates to the day of the year(DOY), then bend the DOY at day 183 to give DOY a seasonal pattern as shown in figure\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mdates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Date'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mdoy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrptime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcdate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'%Y-%m-%d'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimetuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtm_yday\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcdate\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdates\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mdoy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m183\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoy\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m183\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;31m# Load features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pytorch/lib/python3.7/_strptime.py\u001b[0m in \u001b[0;36m_strptime_datetime\u001b[0;34m(cls, data_string, format)\u001b[0m\n\u001b[1;32m    575\u001b[0m     \"\"\"Return a class cls instance based on the input string and the\n\u001b[1;32m    576\u001b[0m     format string.\"\"\"\n\u001b[0;32m--> 577\u001b[0;31m     \u001b[0mtt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfraction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgmtoff_fraction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_strptime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_string\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    578\u001b[0m     \u001b[0mtzname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgmtoff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfraction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pytorch/lib/python3.7/_strptime.py\u001b[0m in \u001b[0;36m_strptime\u001b[0;34m(data_string, format)\u001b[0m\n\u001b[1;32m    354\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"stray %% in format '%s'\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0m_regex_cache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformat_regex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m     \u001b[0mfound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformat_regex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_string\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfound\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m         raise ValueError(\"time data %r does not match format %r\" %\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "sim_type = 'full_wsl'\n",
    "rversion = 'hs32'\n",
    "\n",
    "train_years = 160\n",
    "res_dir = '../results/head_water_SWAT_1000_years/'\n",
    "\n",
    "\n",
    "for output_vars, run_task in [(['SW_ENDmm'],'Train_SMB_inf_SSIF_SW'),\n",
    "                              (['SNOmm'],'Train_SMB_inf_SSIF_SNO'),\n",
    "                              (['Q_pred_mm'],'Train_SMB_inf_SSIF_SF')]:\n",
    "\n",
    "    pred_test_itrs_data = []\n",
    "    hiddens_test_itrs_data = []\n",
    "    mean_rmse_data = []\n",
    "    stdv_rmse_data = []\n",
    "    mean_nse_data = []\n",
    "    stdv_nse_data = []\n",
    "\n",
    "    pred_test_itrs_data_SSIF = []\n",
    "    hiddens_test_itrs_data_SSIF = []\n",
    "    mean_rmse_data_SSIF = []\n",
    "    stdv_rmse_data_SSIF = []\n",
    "    mean_nse_data_SSIF = []\n",
    "    stdv_nse_data_SSIF = []\n",
    "\n",
    "    for n_steps in [14, 30, 90, 180, 366]:\n",
    "        exp_dir = res_dir + '{}/rversion_{}_years_{}_winsize_{}/'.format(run_task, rversion, train_years, n_steps)\n",
    "        if n_steps == 366:\n",
    "            exp_dir = res_dir + '{}/rversion_{}_years_{}/'.format(run_task, rversion, train_years)\n",
    "\n",
    "        #--------------------------------------------- load input data -----------------------------\n",
    "        new_data_dir = '../data/1000_year_simulation/'\n",
    "        if sim_type =='full_wsl':\n",
    "            path = new_data_dir + 'head_water_SWAT_1000_years.csv'\n",
    "        elif sim_type =='nosnow_nofrozen_wsl':\n",
    "            path = new_data_dir + 'head_water_SWAT_1000_years_no_snow_no_frozen.csv'\n",
    "        elif sim_type =='nosnow_wsl':\n",
    "            path = new_data_dir + 'head_water_SWAT_1000_years_no_snow.csv'\n",
    "        else:\n",
    "            raise FileNotFoundError\n",
    "\n",
    "        run_iter = 5\n",
    "        max_iter = run_iter\n",
    "        hidden_size = 32\n",
    "        epochs = 500\n",
    "        learning_rate = 0.01\n",
    "        n_classes = len(output_vars)\n",
    "        batch_size = 64\n",
    "        input_size = 7\n",
    "        dropout = 0\n",
    "        nlayers=1\n",
    "        patience = 50\n",
    "\n",
    "        input_vars = ['Date', 'PRECIPmm', 'TMP_MXdgC', 'TMP_MNdgC', 'SOLARMJ/m2', 'WINDM/s', 'RHmd']\n",
    "        assert(len(input_vars) == input_size)\n",
    "\n",
    "        params = np.load(exp_dir + 'params_ss_{}.npy'.format(n_steps),allow_pickle=True).tolist()\n",
    "        df = pd.read_csv(path)\n",
    "        feat, label = load_data(df, output_vars, input_vars, input_size)\n",
    "\n",
    "        # First 50% of data as the training set, middle 10% of data as the validation set, last 40% of data as the testing set.\n",
    "        train_percentage = 0.5\n",
    "        valid_percentage = 0.1\n",
    "        test_percentage = 1 - (train_percentage + valid_percentage)\n",
    "\n",
    "        # Split data\n",
    "        T = feat.shape[0]\n",
    "        train_len = int(T*train_percentage)\n",
    "        valid_len = int(T*valid_percentage)\n",
    "        test_len = T - train_len - valid_len\n",
    "        print(train_len,valid_len,test_len)\n",
    "        train_x = feat[:train_len].copy()\n",
    "        train_y = label[:train_len].copy()\n",
    "        valid_x = feat[train_len:train_len+valid_len].copy()\n",
    "        valid_y = label[train_len:train_len+valid_len].copy()\n",
    "        test_x = feat[train_len+valid_len:].copy()\n",
    "        test_y = label[train_len+valid_len:].copy()\n",
    "\n",
    "        # Normalize data\n",
    "        scaler_x = StandardScaler()\n",
    "        scaler_x.fit(train_x) \n",
    "        x_train = scaler_x.transform(train_x)\n",
    "        x_valid = scaler_x.transform(valid_x)\n",
    "        x_test = scaler_x.transform(test_x)\n",
    "        scaler_y = StandardScaler()\n",
    "        scaler_y.fit(train_y)\n",
    "        y_train = scaler_y.transform(train_y)\n",
    "        y_valid = scaler_y.transform(valid_y)\n",
    "        y_test = scaler_y.transform(test_y)\n",
    "\n",
    "        # Sample data\n",
    "        ## Get indexes\n",
    "        train_idx = np.arange(len(y_train))\n",
    "        valid_idx = np.arange(len(y_valid))\n",
    "        test_idx = np.arange(len(y_test))\n",
    "        ## Set stride\n",
    "        num_samples_train = 0\n",
    "        shift_train = int(n_steps / 2)\n",
    "        num_samples_valid = 0\n",
    "        shift_valid = int(n_steps / 2)\n",
    "        num_samples_test = 0\n",
    "        shift_test = int(n_steps / 2)\n",
    "        ## Get lists of indexes to sample data. \n",
    "        train_idx_arr = SampleData(train_idx,n_steps,shift_train,num_samples_train)\n",
    "        num_train_samples = train_idx_arr.shape[0]\n",
    "        valid_idx_arr = SampleData(valid_idx,n_steps,shift_valid,num_samples_valid)\n",
    "        num_valid_samples = valid_idx_arr.shape[0]\n",
    "        test_idx_arr = SampleData(test_idx,n_steps,shift_test,num_samples_test)\n",
    "        num_test_samples = test_idx_arr.shape[0]\n",
    "        ## Sample data\n",
    "        x_train_sp_ = x_train[train_idx_arr,:]\n",
    "        y_train_sp_ = y_train[train_idx_arr,:]\n",
    "        m_train_sp_ = y_train_sp_.copy()\n",
    "        m_train_sp_[:,:,:] = 1 # no masking\n",
    "        x_valid_sp_ = x_valid[valid_idx_arr,:]\n",
    "        y_valid_sp_ = y_valid[valid_idx_arr,:]\n",
    "        m_valid_sp_ = y_valid_sp_.copy()\n",
    "        m_valid_sp_[:,:,:] = 1 # no masking\n",
    "        x_test_sp_ = x_test[test_idx_arr,:]\n",
    "        y_test_sp_ = y_test[test_idx_arr,:]\n",
    "        m_test_sp_ = y_test_sp_.copy()\n",
    "        m_test_sp_[:,:,:] = 1 # no masking\n",
    "\n",
    "        # Send data to the device\n",
    "        x_train_sp = torch.from_numpy(x_train_sp_).type(torch.float32).to(device)\n",
    "        y_train_sp = torch.from_numpy(y_train_sp_).type(torch.float32).to(device)\n",
    "        m_train_sp = torch.from_numpy(m_train_sp_).type(torch.float32).to(device)\n",
    "\n",
    "        x_valid_sp = torch.from_numpy(x_valid_sp_).type(torch.float32).to(device)\n",
    "        y_valid_sp = torch.from_numpy(y_valid_sp_).type(torch.float32).to(device)\n",
    "        m_valid_sp = torch.from_numpy(m_valid_sp_).type(torch.float32).to(device)\n",
    "\n",
    "        x_test_sp = torch.from_numpy(x_test_sp_).type(torch.float32).to(device)\n",
    "        y_test_sp = torch.from_numpy(y_test_sp_).type(torch.float32).to(device)\n",
    "        m_test_sp = torch.from_numpy(m_test_sp_).type(torch.float32).to(device)\n",
    "\n",
    "        # SSIF Inference #####################################################\n",
    "        print(\"SSIF Inference ###\")\n",
    "        pred_test_itrs = []\n",
    "        hiddens_test_itrs = []\n",
    "        rmses_test_itrs = []\n",
    "\n",
    "        for run_iter in np.arange(max_iter):\n",
    "            # print('Iteration {}'.format(run_iter))\n",
    "            # Load the best validation model\n",
    "            path_save = exp_dir+\"run_iter_{}_best_model.sav\".format(run_iter)\n",
    "            checkpoint=torch.load(path_save)\n",
    "            model_trained=GRU(input_size, hidden_size, nlayers, n_classes, dropout)\n",
    "            model_trained.load_state_dict(checkpoint['model_state_dict'])\n",
    "            model_trained.to(device)\n",
    "            epoch = checkpoint['epoch']\n",
    "            # print(\"Best epoch is {}\".format(epoch))\n",
    "\n",
    "            with torch.no_grad():\n",
    "                model_trained.eval()\n",
    "                hidden_head = model_trained.init_hidden(1)\n",
    "                X = torch.from_numpy(np.expand_dims(x_test,0)).type(torch.float32).to(device)\n",
    "                pred_test,hiddens_test = model_trained(X,hidden_head)\n",
    "                pred_test = pred_test.cpu().numpy().reshape(-1,1)\n",
    "                hiddens_test = np.squeeze(hiddens_test.cpu().numpy())\n",
    "                pred_test = scaler_y.inverse_transform(pred_test)\n",
    "                loss_test = MSE(pred_test, test_y)\n",
    "                rmses_test_itrs.append(np.sqrt(loss_test))\n",
    "                # print(\"Epoch {} : epoch_loss_test RMSE loss {:.4f}\".format(str(epoch), np.sqrt(loss_test)))\n",
    "                # print()\n",
    "            pred_test_itrs.append(pred_test)\n",
    "            hiddens_test_itrs.append(hiddens_test)\n",
    "\n",
    "        with open(exp_dir + \"pred_test_agg_stateful_itrs.npy\", 'wb') as f:\n",
    "            np.save(f, pred_test_itrs)\n",
    "\n",
    "        rmses_test_itrs = np.asarray(rmses_test_itrs)\n",
    "        print(\"Test RMSE Mean\")\n",
    "        print(np.round(rmses_test_itrs,3))\n",
    "        print(\"Test RMSE Mean : {:.3f}\".format(np.mean(rmses_test_itrs)))\n",
    "        print(\"Test RMSE stdv : {:.3f}\".format(np.std(rmses_test_itrs)))  \n",
    "\n",
    "        nses_test_itrs = []\n",
    "        for pred_test in pred_test_itrs:\n",
    "            nses_test_itrs.append(get_NSE(test_y, pred_test))\n",
    "\n",
    "        nses_test_itrs = np.asarray(nses_test_itrs)\n",
    "        print(\"Test NSE\")\n",
    "        print(np.round(nses_test_itrs,3))\n",
    "        print(\"Test NSE Mean : {:.3f}\".format(np.mean(nses_test_itrs)))\n",
    "        print(\"Test NSE stdv : {:.3f}\".format(np.std(nses_test_itrs)))\n",
    "\n",
    "        pred_test_itrs_data_SSIF.append(pred_test_itrs)\n",
    "        hiddens_test_itrs_data_SSIF.append(hiddens_test_itrs) \n",
    "        mean_rmse_data_SSIF.append([n_steps,np.mean(rmses_test_itrs)])\n",
    "        stdv_rmse_data_SSIF.append([n_steps,np.std(rmses_test_itrs)])\n",
    "        mean_nse_data_SSIF.append([n_steps,np.mean(nses_test_itrs)])\n",
    "        stdv_nse_data_SSIF.append([n_steps,np.std(nses_test_itrs)])   \n",
    "\n",
    "    exp_dir = res_dir + '{}/rversion_{}/'.format(run_task, rversion)\n",
    "\n",
    "    # results_data = {'pred_test_itrs_data':np.squeeze(pred_test_itrs_data_SSIF),\n",
    "    #                'mean_rmse_data':np.squeeze(mean_rmse_data_SSIF), 'stdv_rmse_data':stdv_rmse_data_SSIF,\n",
    "    #                'mean_nse_data':np.squeeze(mean_nse_data_SSIF), 'stdv_nse_data':stdv_nse_data_SSIF}\n",
    "    results_data = {'mean_rmse_data':np.squeeze(mean_rmse_data_SSIF), 'stdv_rmse_data':stdv_rmse_data_SSIF,\n",
    "                   'mean_nse_data':np.squeeze(mean_nse_data_SSIF), 'stdv_nse_data':stdv_nse_data_SSIF}\n",
    "    \n",
    "    with open(exp_dir+'results_winsize_SSIF.pickle', 'wb') as handle:\n",
    "        pickle.dump(results_data, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    mean_rmse_data = np.squeeze(mean_rmse_data)\n",
    "    stdv_rmse_data = np.squeeze(stdv_rmse_data)\n",
    "    mean_rmse_data_SSIF = np.squeeze(mean_rmse_data_SSIF)\n",
    "    stdv_rmse_data_SSIF = np.squeeze(stdv_rmse_data_SSIF)\n",
    "\n",
    "    fig = plt.figure()\n",
    "    plt.errorbar(mean_rmse_data_SSIF[:,0], mean_rmse_data_SSIF[:,1], yerr=stdv_rmse_data_SSIF[:,1], label='SSIF')\n",
    "    # plt.title('RMSE of models')\n",
    "    plt.xlabel('RNN lengths')\n",
    "    plt.ylabel('RMSE')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_rmse_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_a100",
   "language": "python",
   "name": "pytorch_a100"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
