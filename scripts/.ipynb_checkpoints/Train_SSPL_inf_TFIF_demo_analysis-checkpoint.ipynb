{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import math\n",
    "import sklearn.preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "import subprocess as sp\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "np.random.seed(4)\n",
    "\n",
    "from models import EarlyStopping, GRU\n",
    "from utilities import get_NSE, MSE, get_colors, print_notes, inverse_transform_sp\n",
    "from preprocess import load_data, SampleData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "def get_gpu_memory():\n",
    "    _output_to_list = lambda x: x.decode('ascii').split('\\n')[:-1]\n",
    "    ACCEPTABLE_AVAILABLE_MEMORY = 1024\n",
    "    COMMAND = \"nvidia-smi --query-gpu=memory.free --format=csv\"\n",
    "    memory_free_info = _output_to_list(sp.check_output(COMMAND.split()))[1:]\n",
    "    memory_free_values = [int(x.split()[0]) for i, x in enumerate(memory_free_info)]\n",
    "    return memory_free_values\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "if get_gpu_memory()[0] < 1500 :\n",
    "    device = torch.device(\"cpu\")\n",
    "print(device)\n",
    "\n",
    "# Get colors\n",
    "colors = get_colors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the comments to run the SWAT-SW experiment.\n",
    "# output_vars = ['SW_ENDmm']\n",
    "# run_task = 'Train_SSPL_inf_TFIF_SW'\n",
    "\n",
    "## Remove the comments to run the SWAT-SNO experiment.\n",
    "# output_vars = ['SNOmm']\n",
    "# run_task = 'Train_SSPL_inf_TFIF_SNO'\n",
    "\n",
    "# ## Remove the comments to run the SWAT-SF experiment.\n",
    "output_vars = ['Q_pred_mm']\n",
    "run_task = 'Train_SSPL_inf_TFIF_SF'\n",
    "\n",
    "sim_type = 'full_wsl'\n",
    "rversion = 'hs32'\n",
    "#----------------paths-------------------\n",
    "res_dir = '../results/head_water_SWAT_1000_years/'\n",
    "exp_dir = res_dir + '{}/rversion_{}/'.format(run_task, rversion)\n",
    "\n",
    "#--------------------------------------------- load input data -----------------------------\n",
    "new_data_dir = '../data/1000_year_simulation/'\n",
    "if sim_type =='full_wsl':\n",
    "    path = new_data_dir + 'head_water_SWAT_1000_years.csv'\n",
    "elif sim_type =='nosnow_nofrozen_wsl':\n",
    "    path = new_data_dir + 'head_water_SWAT_1000_years_no_snow_no_frozen.csv'\n",
    "elif sim_type =='nosnow_wsl':\n",
    "    path = new_data_dir + 'head_water_SWAT_1000_years_no_snow.csv'\n",
    "else:\n",
    "    raise FileNotFoundError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The network hypermeters : \n",
      "learning_rate 0.01\n",
      "epochs 500\n",
      "batch_size 64\n",
      "hidden_size 32\n",
      "input_size 8\n",
      "n_steps 366\n",
      "dropout 0.0\n",
      "n_classes 1\n",
      "num_samples_train 0\n",
      "shift_train 183\n",
      "num_samples_valid 0\n",
      "shift_valid 183\n",
      "num_samples_test 0\n",
      "shift_test 183\n",
      "train_percentage 0.5\n",
      "valid_percentage 0.1\n",
      "\n",
      "Goal: Simulate the target variable using 7 weather drivers including Date, PRECIPmm, TMP_MXdgC, TMP_MNdgC, SOLARMJ/m2, WINDM/s, RHmd\n",
      "Target variables: SW_ENDmm, SNOmm, Q_pred_mm\n",
      "Data: 1000 years simulation data\n",
      "Split data: First 50% of data as the training set, middle 10% of data as the validation set,  last 40% of data as the testing set\n",
      "Settings:\n",
      "- Training: Random mini-batch (RMB) algorithm\n",
      "- Model: one layer GRU\n",
      "\n"
     ]
    }
   ],
   "source": [
    "run_iter = 5\n",
    "max_iter = run_iter\n",
    "n_steps = 366\n",
    "hidden_size = 32\n",
    "epochs = 500\n",
    "learning_rate = 0.01\n",
    "n_classes = len(output_vars)\n",
    "batch_size = 64\n",
    "input_size = 7\n",
    "dropout = 0\n",
    "nlayers=1\n",
    "patience = 50\n",
    "\n",
    "input_vars = ['Date', 'PRECIPmm', 'TMP_MXdgC', 'TMP_MNdgC', 'SOLARMJ/m2', 'WINDM/s', 'RHmd']\n",
    "assert(len(input_vars) == input_size)\n",
    "params = np.load(exp_dir + 'params_ss_{}.npy'.format(n_steps),allow_pickle=True).tolist()\n",
    "print(\"The network hypermeters : \")\n",
    "for k,v in params.items():\n",
    "    if k != \"notes\":\n",
    "        print(k,v)\n",
    "    if k == \"notes\":\n",
    "        print()\n",
    "        print_notes(params['notes'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(path)\n",
    "feat, label = load_data(df, output_vars, input_vars, input_size)\n",
    "label_shift = np.expand_dims([label[0,0]] + list(np.squeeze(label[:-1])), axis=1)\n",
    "feat = np.concatenate((feat, label_shift), axis=-1)\n",
    "input_size = input_size + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "182622 36524 146098\n",
      "182622\n",
      "36524\n",
      "146098\n"
     ]
    }
   ],
   "source": [
    "# First 50% of data as the training set, middle 10% of data as the validation set, last 40% of data as the testing set.\n",
    "train_percentage = 0.5\n",
    "valid_percentage = 0.1\n",
    "test_percentage = 1 - (train_percentage + valid_percentage)\n",
    "\n",
    "# Split data\n",
    "T = feat.shape[0]\n",
    "train_len = int(T*train_percentage)\n",
    "valid_len = int(T*valid_percentage)\n",
    "test_len = T - train_len - valid_len\n",
    "print(train_len,valid_len,test_len)\n",
    "train_x = feat[:train_len].copy()\n",
    "train_y = label[:train_len].copy()\n",
    "valid_x = feat[train_len:train_len+valid_len].copy()\n",
    "valid_y = label[train_len:train_len+valid_len].copy()\n",
    "test_x = feat[train_len+valid_len:].copy()\n",
    "test_y = label[train_len+valid_len:].copy()\n",
    "\n",
    "# Normalize data\n",
    "scaler_x = StandardScaler()\n",
    "scaler_x.fit(train_x) \n",
    "x_train = scaler_x.transform(train_x)\n",
    "x_valid = scaler_x.transform(valid_x)\n",
    "x_test = scaler_x.transform(test_x)\n",
    "scaler_y = StandardScaler()\n",
    "scaler_y.fit(train_y)\n",
    "y_train = scaler_y.transform(train_y)\n",
    "y_valid = scaler_y.transform(valid_y)\n",
    "y_test = scaler_y.transform(test_y)\n",
    "m_test = y_test.copy()\n",
    "m_test[:,:] = 1\n",
    "\n",
    "# Sample data\n",
    "## Get indexes\n",
    "train_idx = np.arange(len(y_train))\n",
    "valid_idx = np.arange(len(y_valid))\n",
    "test_idx = np.arange(len(y_test))\n",
    "## Set stride\n",
    "num_samples_train = 0\n",
    "shift_train = int(n_steps / 2)\n",
    "num_samples_valid = 0\n",
    "shift_valid = int(n_steps / 2)\n",
    "num_samples_test = 0\n",
    "shift_test = int(n_steps / 2)\n",
    "## Get lists of indexes to sample data. \n",
    "train_idx_arr = SampleData(train_idx,n_steps,shift_train,num_samples_train)\n",
    "num_train_samples = train_idx_arr.shape[0]\n",
    "valid_idx_arr = SampleData(valid_idx,n_steps,shift_valid,num_samples_valid)\n",
    "num_valid_samples = valid_idx_arr.shape[0]\n",
    "test_idx_arr = SampleData(test_idx,n_steps,shift_test,num_samples_test)\n",
    "num_test_samples = test_idx_arr.shape[0]\n",
    "## Sample data\n",
    "x_train_sp_ = x_train[train_idx_arr,:]\n",
    "y_train_sp_ = y_train[train_idx_arr,:]\n",
    "m_train_sp_ = y_train_sp_.copy()\n",
    "m_train_sp_[:,:,:] = 1 # no masking\n",
    "x_valid_sp_ = x_valid[valid_idx_arr,:]\n",
    "y_valid_sp_ = y_valid[valid_idx_arr,:]\n",
    "m_valid_sp_ = y_valid_sp_.copy()\n",
    "m_valid_sp_[:,:,:] = 1 # no masking\n",
    "x_test_sp_ = x_test[test_idx_arr,:]\n",
    "y_test_sp_ = y_test[test_idx_arr,:]\n",
    "m_test_sp_ = y_test_sp_.copy()\n",
    "m_test_sp_[:,:,:] = 1 # no masking\n",
    "\n",
    "# Send data to the device\n",
    "x_train_sp = torch.from_numpy(x_train_sp_).type(torch.float32).to(device)\n",
    "y_train_sp = torch.from_numpy(y_train_sp_).type(torch.float32).to(device)\n",
    "m_train_sp = torch.from_numpy(m_train_sp_).type(torch.float32).to(device)\n",
    "\n",
    "x_valid_sp = torch.from_numpy(x_valid_sp_).type(torch.float32).to(device)\n",
    "y_valid_sp = torch.from_numpy(y_valid_sp_).type(torch.float32).to(device)\n",
    "m_valid_sp = torch.from_numpy(m_valid_sp_).type(torch.float32).to(device)\n",
    "\n",
    "x_test_sp = torch.from_numpy(x_test_sp_).type(torch.float32).to(device)\n",
    "y_test_sp = torch.from_numpy(y_test_sp_).type(torch.float32).to(device)\n",
    "m_test_sp = torch.from_numpy(m_test_sp_).type(torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = torch.from_numpy(x_test).type(torch.float32).to(device)\n",
    "y_test = torch.from_numpy(y_test).type(torch.float32).to(device)\n",
    "m_test = torch.from_numpy(m_test).type(torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([146098, 1])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n",
      "Total epoch : 499\n",
      "Total training time : 23.7422\n",
      "Iteration 1\n",
      "Total epoch : 499\n",
      "Total training time : 26.2978\n",
      "Iteration 2\n",
      "Total epoch : 499\n",
      "Total training time : 25.0057\n",
      "Iteration 3\n",
      "Total epoch : 499\n",
      "Total training time : 26.8451\n",
      "Iteration 4\n",
      "Total epoch : 499\n",
      "Total training time : 29.7513\n",
      "0.0528 second/epoch\n"
     ]
    }
   ],
   "source": [
    "# Analyze the training time\n",
    "tot_epoch = 0\n",
    "tot_time = 0\n",
    "for run_iter in np.arange(max_iter):\n",
    "    print('Iteration {}'.format(run_iter))\n",
    "    # # Load the final epoch model\n",
    "    path_save = exp_dir+\"run_iter_{}_final_model.sav\".format(run_iter)\n",
    "    checkpoint=torch.load(path_save)\n",
    "    epoch = checkpoint['epoch']\n",
    "    training_time = checkpoint['train_time']\n",
    "    tot_epoch += epoch\n",
    "    tot_time += training_time\n",
    "    print(\"Total epoch : {}\".format(epoch))\n",
    "    print(\"Total training time : {:.4f}\".format(training_time))\n",
    "    \n",
    "print(\"{:.4f} second/epoch\".format(tot_time / tot_epoch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SSPL-TFIF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n",
      "Best epoch is 240\n",
      "0\n",
      "40000\n",
      "80000\n",
      "120000\n",
      "Iteration 1\n",
      "Best epoch is 225\n",
      "0\n",
      "40000\n",
      "80000\n"
     ]
    }
   ],
   "source": [
    "pred_test_itrs = []\n",
    "select = \"best\"\n",
    "for run_iter in np.arange(max_iter):\n",
    "    print('Iteration {}'.format(run_iter))\n",
    "    # Load the best validation model\n",
    "    model_trained=GRU(input_size, hidden_size, nlayers, n_classes, dropout)\n",
    "    if select == \"best\":\n",
    "        path_save = exp_dir+\"run_iter_{}_best_model.sav\".format(run_iter)\n",
    "        checkpoint=torch.load(path_save)\n",
    "        model_trained.load_state_dict(checkpoint['model_state_dict'])\n",
    "    if select == \"final\":\n",
    "        path_save = exp_dir+\"run_iter_{}_final_model.sav\".format(run_iter)\n",
    "        checkpoint=torch.load(path_save)\n",
    "        model_trained.load_state_dict(checkpoint['model_state_dict_fs'])\n",
    "    \n",
    "    model_trained.to(device)\n",
    "    epoch = checkpoint['epoch']\n",
    "    print(\"Best epoch is {}\".format(epoch))\n",
    "    pred_test_list = torch.zeros(y_test.shape).type(torch.float32).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model_trained.eval()\n",
    "        hidden_head = model_trained.init_hidden(1)\n",
    "        for i in range(len(x_test)):\n",
    "            pred_test,hiddens_test = model_trained(x_test[i][None,None,:],hidden_head)\n",
    "            hidden_head = hiddens_test\n",
    "            if i < len(x_test) - 1:\n",
    "                x_test[i+1,-1] = pred_test[0,0,0]\n",
    "            pred_test_list[i,0] = pred_test[0,0,0]\n",
    "            if i % 40000 == 0:\n",
    "                print(i)\n",
    "\n",
    "        loss_test = MSE(pred_test_list, y_test, m_test).cpu().numpy()\n",
    "        pred_test = pred_test_list.cpu().numpy()\n",
    "        hiddens_test = hiddens_test.cpu().numpy()\n",
    "    \n",
    "    # Transform the output back to the original scale.\n",
    "    pred_test, y_test_ori = inverse_transform_sp(pred_test, y_test.cpu().numpy(), scaler_y, n_classes)\n",
    "    pred_test_itrs.append(pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test_itrs = np.squeeze(pred_test_itrs)\n",
    "rmse_test_itrs = []\n",
    "nse_test_itrs=[]\n",
    "\n",
    "for pred_test in pred_test_itrs:\n",
    "    rmse_test_itrs.append(np.sqrt(MSE(pred_test, np.squeeze(test_y))))\n",
    "    nse_test_itrs.append(get_NSE(np.squeeze(test_y), pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(exp_dir + \"pred_test_agg_stateful_itrs.npy\", 'wb') as f:\n",
    "    np.save(f, pred_test_itrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test RMSE Mean\")\n",
    "print(np.round(rmse_test_itrs,3))\n",
    "print(\"Test RMSE Mean : {:.3f}\".format(np.mean(rmse_test_itrs)))\n",
    "print(\"Test RMSE STDV : {:.3f}\".format(np.std(rmse_test_itrs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test NSE\")\n",
    "print(np.round(nse_test_itrs,3))\n",
    "print(\"Test NSE Mean : {:.3f}\".format(np.mean(nse_test_itrs)))\n",
    "print(\"Test NSE STDV : {:.3f}\".format(np.std(nse_test_itrs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test_agg = pred_test_itrs[0]\n",
    "pred_test_agg = pred_test_agg[:, None]\n",
    "output_vars = np.asarray(output_vars)\n",
    "output_var = output_vars[0]\n",
    "idx = np.where(output_vars == output_var)[0][0]\n",
    "pred_agg = pred_test_agg[:,idx]\n",
    "y = test_y[:,idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low = 366*0\n",
    "high = 366*3\n",
    "period = np.arange(low,high)\n",
    "pred_snippet = pred_agg[period]\n",
    "y_snippet = y[period]\n",
    "\n",
    "ig,ax = plt.subplots(1,1, figsize=(18,3))\n",
    "ax.plot(period,pred_snippet,c='r',alpha=0.8, label='prd')\n",
    "ax.plot(period,y_snippet,c='black',alpha=0.8, label='truth')\n",
    "plt.title(\"Test, Period {} to {}, RMSE {:.4f}\".format(low,high, np.sqrt(MSE(pred_snippet, y_snippet))))\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_a100",
   "language": "python",
   "name": "pytorch_a100"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
